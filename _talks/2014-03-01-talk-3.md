---
title: "Convex-Concave Minmax Optimization: Applications and Methods"
collection: talks
type: "Seminar"
permalink: /talks/2022-08-05-minmax
venue: "Shenzhen Research Institute of Big Data (SRIBD) Forum"
date: 2022-08-05
location: "Shenzhen, China"
---

In this seminar, I first introduced the smooth convex-concave saddle point problem and its intuitions. To solve such problem, I intuitively showed an algorithm called gradient descent-ascent (GDA) that theoretically feasible but practically diverged, and further showing its converged variant, proximal point algorithm (PPA). Given the intractability of PPA's future step gradient $\nabla f(x_{k+1},y_{k+1})$, I provided the optimistic gradient descent-ascent algorithm (OGDA) and the extragradient (EG) algorithm, and highlighted how gradients used in OGDA and EG approximate the gradient of the PPA. Then, I exploit this interpretation to show that the primal-dual gap of the averaged iterates generated by both algorithms converge with a rate of $O(1/k)$. Ultimately, I analyzed the last iterate convergence properties of both algorithms, and showed that the last iterate of both algorithms converge at a rate of $O(1/\sqrt{k})$, which is slower than the averaged iterate in smooth convex-concave saddle point problem.

**Seminar slides**: \[[slides](https://cuhko365-my.sharepoint.com/:b:/g/personal/221025012_link_cuhk_edu_cn/EcH23PH0EhtGgpnw9iVREFAB56areWk9FcvTFD-7CpX9jQ)\].
<br/>
**Seminar notes**: \[[notebook](https://github.com/yilingu0094/yilingu0094.github.io/tree/master/files/minmax_file/notebook_minmax_gu.pdf)\].
<br/>
**Seminar codes**: \[[codes](https://github.com/yilingu0094/yilingu0094.github.io/tree/master/files/minmax_codes)\].

**Reference Reading Materials**:

1. Golowich, Noah, et al. "Last iterate is slower than averaged iterate in smooth convex-concave saddle point problems." *Conference on Learning Theory*. PMLR, 2020.\[[pdf](https://yilingu0094.github.io/files/minmax_file/Last Iterate is Slower than Averaged Iterate in Smooth convex-concave saddle point problems.pdf)\].

2. Mokhtari, Aryan, Asuman E. Ozdaglar, and Sarath Pattathil. "Convergence Rate of $O(1/k)$ for Optimistic Gradient and Extragradient Methods in Smooth Convex-Concave Saddle Point Problems." *SIAM Journal on Optimization* 30.4 (2020): 3230-3251.\[[pdf](https://yilingu0094.github.io/files/minmax_file/19m127375x.pdf)\].

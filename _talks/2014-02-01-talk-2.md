---
title: "SAGA: Introduction to Variance Reduction"
collection: talks
type: "Seminar"
permalink: /talks/2022-02-18-SAGA
venue: "Shenzhen Research Institute of Big Data (SRIBD) Forum"
date: 2022-02-18
location: "Shenzhen, China"
---

This seminar is about a recursive framework for improving convergence performance in expectation on convex stochastic optimization. By replacing the gradient of the reference point with the last iterate, the stochastic average gradient algorithm (SAGA) saves more computational resource with linear convergence, and supports for composite objectives where a proximal operator is used on the regularizer, compared with stochastic variance reduced gradients (SVRG).

**Seminar slides**: \[[slides](https://cuhko365-my.sharepoint.com/:b:/g/personal/221025012_link_cuhk_edu_cn/Ecwkm348nqlNqwqSTuQEeTkBvo3zf80GA7GPpbjosY6BwQ?e=VjVIxZ)\].

**Reference Reading Materials**:

1. Defazio, Aaron, Francis Bach, and Simon Lacoste-Julien. "SAGA: A fast incremental gradient method with support for non-strongly convex composite objectives." *Advances in neural information processing systems* 27 (2014).\[[pdf](https://yilingu0094.github.io/files/minmax_file/Defazio_NIPS2014.pdf)\].
